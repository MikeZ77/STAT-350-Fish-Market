---
output:
  pdf_document: default
  html_document: default
---
# Risidual Analysis

_**Five important assumptions need to hold so that the regression model can be useful hypothesis testing and predication. These are:**_

1. The relationship between the response y and the regression is linear (at least approximately).
2. The error term $\epsilon$ has zero mean.
3. The error term $\epsilon$ has constant variance $\sigma^{2}$.
4. The errors are uncorrelated.
5. The errors are normally distributed.

```{r}
data <- read.csv("cleaned_data_scaled_only_3.csv", fileEncoding="UTF-8-BOM")

```

Lets use the risidual plots using standardized residuals so that we can compare the current state of the model with these assumptions.

```{r}

model <- lm(Weight ~ Length1 + Height + Width - 1, data = data)
  
  par(mfrow = c(2, 2))
  plot(model)

```

## Risiduals vs Fitted

Here, assumption 1. is violated (we want to see a linear pattern between the Risiduals and Fitted values). This is not surprising since we observed a non linear pattern between the predictors and response in the pairs plot.

```{r}

model_transformed <- lm(sqrt(Weight) ~ Length1 + Height + Width - 1, data = data)
  
plot(model_transformed, which = 1)

```

Taking the square root of the response seems to produce the best result compared to other transformations of the response like ln(). Drawing a horizontal line at 0 seems reasonable. This satisfies 1. The relationship between the response y and the regression is linear. To confirm that we now have an improved linear relationship, we can compare the $R^{2}$ of the model.

```{r}

model <- lm(Weight ~ Length1 + Height + Width - 1, data = data)
summary(model)
summary(model_transformed)

```

Performing this transformation has significantly improved the $R^{2}$ of the model from `r {summary(model)$r.squared}` to `r {summary(model_transformed)$r.squared}`.

## Normal Q-Q

```{r}

plot(model_transformed, which = 2)

```

<!-- ## 1. The relationship between the response y and the regression is linear -->

<!-- Lets plot each predictor separately to determine the relationship and transform it to a linear relationship if necessary. -->

<!-- ```{r} -->
<!-- par(mfrow=c(2,2)) -->
<!-- plot(data$Length1,data$Weight) -->
<!-- plot(data$Width,data$Weight) -->
<!-- plot(data$Height,data$Weight) -->

<!-- ``` -->

<!-- All predictors do not have a linear relationship, and all seem to have more of an exponential fit. It makes sense to try an linearize the model using a ln(y) transformation. -->

<!-- ```{r} -->
<!-- data$Weight <-sqrt(data$Weight)  -->

<!-- par(mfrow=c(2,2)) -->
<!-- plot(data$Length1,data$Weight) -->
<!-- plot(data$Width,data$Weight) -->
<!-- plot(data$Height,data$Weight) -->

<!-- ``` -->



